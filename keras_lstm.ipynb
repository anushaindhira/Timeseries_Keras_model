{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras_timeseries.py\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten    \n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import optimizers\n",
    "#from keras import backend as K\n",
    "\n",
    "df = pd.read_csv(\"customers_timeseries_updated.csv\")\n",
    "#df = pd.read_csv(\"data-1000customers.csv\")\n",
    "#df = pd.read_csv(\"100_timeseries_checking_to_share.csv\")\n",
    "#print (df.columns.values)\n",
    "#df = df.iloc[:,0:3]\n",
    "#use column names\n",
    "df = df[['client_debtor_number','dates','fv_cost']]\n",
    "#print(\"new dataset columns \",df.columns.values)\n",
    "#df.head()\n",
    "# number of customers\n",
    "len(df['client_debtor_number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1015130 = df[df['client_debtor_number'] == 1015193]\n",
    "del df_1015130['client_debtor_number']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data  0           0.000000\n",
      "1           0.000000\n",
      "2           0.000000\n",
      "3           0.000000\n",
      "4           0.000000\n",
      "5           0.000000\n",
      "6           0.000000\n",
      "7           0.000000\n",
      "8           0.000000\n",
      "9           0.000000\n",
      "10          0.000000\n",
      "11          0.000000\n",
      "12          0.000000\n",
      "13          0.000000\n",
      "14          0.000000\n",
      "15          0.000000\n",
      "16          0.000000\n",
      "17          0.000000\n",
      "18          0.000000\n",
      "19          0.000000\n",
      "20          0.000000\n",
      "21          0.000000\n",
      "22          0.000000\n",
      "23          0.000000\n",
      "24          0.000000\n",
      "25          0.000000\n",
      "26          0.000000\n",
      "27          0.000000\n",
      "28          0.000000\n",
      "29          0.000000\n",
      "             ...    \n",
      "307581    190.301116\n",
      "307582    193.302938\n",
      "307583    196.305162\n",
      "307584    197.021261\n",
      "307585    199.928856\n",
      "307586    191.000154\n",
      "307587    194.007212\n",
      "307588    194.524591\n",
      "307589    197.375700\n",
      "307590    200.080716\n",
      "307591     61.073114\n",
      "307592     62.782689\n",
      "307593     64.492494\n",
      "307594     66.202527\n",
      "307595     67.912788\n",
      "307596     67.921500\n",
      "307597     70.007468\n",
      "307598     71.637040\n",
      "307599     71.879163\n",
      "307600     73.556932\n",
      "307601    351.494657\n",
      "307602    354.894033\n",
      "307603    358.293864\n",
      "307604    361.425029\n",
      "307605    364.835032\n",
      "307606    368.245490\n",
      "307607    371.656404\n",
      "307608    375.067774\n",
      "307609    378.479600\n",
      "307610    381.891882\n",
      "Name: fv_cost, Length: 3192, dtype: float64\n",
      "test_data  152       392.131466\n",
      "153       381.770362\n",
      "154       385.427760\n",
      "155       389.324922\n",
      "156       393.222606\n",
      "157       397.120811\n",
      "158       401.019537\n",
      "159       404.918784\n",
      "160       788.531212\n",
      "161       794.142636\n",
      "162       801.190424\n",
      "163       806.942346\n",
      "164       814.036652\n",
      "165       821.131907\n",
      "166       828.228111\n",
      "167       782.594663\n",
      "168       788.944640\n",
      "169       794.757227\n",
      "16340     408.818552\n",
      "16341     408.818552\n",
      "16342     406.676080\n",
      "16343     408.093676\n",
      "16344     412.295644\n",
      "16345     416.498174\n",
      "16346     420.701265\n",
      "16347     424.904919\n",
      "16348     429.109134\n",
      "16349     433.313911\n",
      "16350     801.127428\n",
      "16351     807.498480\n",
      "             ...    \n",
      "275251    813.232789\n",
      "275252    803.548415\n",
      "275253    808.921157\n",
      "275254    816.033003\n",
      "275255    823.145800\n",
      "275256    823.145800\n",
      "275257    828.225651\n",
      "275258    835.410353\n",
      "275259    835.410353\n",
      "275260    835.410353\n",
      "291431    841.660312\n",
      "291432    848.879140\n",
      "291433    856.098933\n",
      "291434    863.319691\n",
      "291435    870.541415\n",
      "291436    876.956745\n",
      "291437    884.208187\n",
      "291438    891.460599\n",
      "291439    897.381822\n",
      "291440    904.387655\n",
      "307611    798.491611\n",
      "307612    806.747737\n",
      "307613    806.747737\n",
      "307614    815.152290\n",
      "307615    782.783906\n",
      "307616    787.191543\n",
      "307617    777.052551\n",
      "307618    782.484156\n",
      "307619    788.999546\n",
      "307620    781.740596\n",
      "Name: fv_cost, Length: 286, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#df[df.Date.str.match(r'^2018-06')]\n",
    "#df[df[\\\"col\\\"].str.contains('this|that')==False]\n",
    "# splitting train and test datasets \n",
    "#training 2010-2017, testing on 2018 data\\\n",
    "train_x = df_1015130[df_1015130['dates'].str.contains('/2018') == False]\n",
    "test_x = df_1015130[df_1015130['dates'].str.contains('/2018') == True]\n",
    "#print(test_x.dates)\n",
    "train_x_cust = train_x['fv_cost']\n",
    "test_x_cust = test_x['fv_cost']\n",
    "\n",
    "print(\"train_data \",train_x_cust)\n",
    "print(\"test_data \", test_x_cust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://drivendata.co/blog/benchmark-cold-start-lstm-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3192.000000\n",
       "mean       91.482684\n",
       "std       170.405114\n",
       "min      -156.842600\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%       113.278646\n",
       "max       813.317904\n",
       "Name: fv_cost, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x['fv_cost'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print (train_x[train_x['fv_cost'].str.contains(2016)])\n",
    "n_input = 2016\n",
    "\n",
    "train_x[(train_x == n_input).any(1)].stack()[lambda x: x != n_input].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3189, 1, 3)\n",
      "(3189,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lag_feature(df, lag=1):\n",
    "    if not type(df) == pd.DataFrame:\n",
    "        df = pd.DataFrame(df, columns=['fv_cost'])\n",
    "    \n",
    "    def rename_lag(ser, j):\n",
    "        ser.name = ser.name + f'_{j}'\n",
    "        return ser\n",
    "        \n",
    "    # add a column lagged by `i` steps\n",
    "    for i in range(1, lag + 1):\n",
    "        df = df.join(df.fv_cost.shift(i).pipe(rename_lag, i))\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#lagged_data=pd.DataFrame(lag_feature(train_x_cust))\n",
    "\n",
    "# X, y format taking the first column (original time series) to be the y\n",
    "#X = lagged_data.drop('fv_cost', axis=1).values\n",
    "#y = lagged_data.fv_cost.values\n",
    "\n",
    "#train_x_cust = lagged_data[:,0]\n",
    "#train_y_cust = lagged_data[:,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_training_data(series_data, lag):\n",
    "    \" Converts a series of data into a lagged, scaled sample.\"\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    cost_vals = scaler.fit_transform(series_data.values.reshape(-1, 1))\n",
    "    \n",
    "    # convert series to lagged features\n",
    "    cost_lagged = lag_feature(cost_vals, lag=lag)\n",
    "\n",
    "    # X, y format taking the first column (original time series) to be the y\n",
    "    X = cost_lagged.drop('fv_cost', axis=1).values\n",
    "    y = cost_lagged.fv_cost.values\n",
    "    \n",
    "    # keras expects 3 dimensional X\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "train_x_cust,train_y_cust, scalar_train = prepare_training_data(train_x['fv_cost'], 3)\n",
    "test_x_cust,test_y_cust,scalar_test = prepare_training_data(test_x['fv_cost'], 3)\n",
    "print(train_x_cust.shape)\n",
    "print(train_y_cust.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag of 24 to simulate smallest cold start window. Our series\n",
    "# will be converted to a num_timesteps x lag size matrix\n",
    "lag =  3\n",
    "\n",
    "# model parameters\n",
    "num_neurons = 50 #24\n",
    "batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "dropout_rate =0.2\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#add convolution layer\n",
    "\n",
    "#Convolution2D (https://keras.io/layers/convolutional/) expects the input to be in the format (samples, rows, cols, channels), \n",
    "#which is \"channels-last\". data is in the format (samples, channels, rows, cols). You should be able to fix\n",
    "#this using the optional keyword data_format = 'channels_first' when declaring the Convolution2D layer else use\n",
    "# input_shape=(3,1).\n",
    "# when strides>1 you cannot have dilation>1\n",
    "#activation_func = ['softmax', 'softplus', 'softsign', 'relu', \n",
    "#                   'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "model.add(Conv1D(filters=num_neurons,batch_size=1, kernel_size=3,\n",
    "                 strides=3, \n",
    "                 padding=\"same\",activation='linear',dilation_rate=1, \n",
    "                 input_shape=(1, 3),data_format='channels_first'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(dropout_rate))\n",
    "#model.add(Flatten()) # as flatten is converting data into 1D but we need 3D for our 3lags data here\n",
    "model.add(Conv1D(filters=num_neurons,batch_size=1, kernel_size=3,\n",
    "                 strides=3, \n",
    "                 padding=\"same\",activation='linear',dilation_rate=1, \n",
    "                 input_shape=(1,3),data_format='channels_first'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "#model.add(Dropout(dropout_rate))\n",
    "# add LSTM layer - stateful MUST be true here in \n",
    "# order to learn the patterns within a series\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# followed by a dense layer with a single output for regression\n",
    "model.add(Dense(16,activation='linear'))\n",
    "#model.add(Dense(18,activation='exponential'))\n",
    "model.add(Dense(1,activation='linear'))\n",
    "# we can add dropoutlayer after dense as well again\n",
    "# compile\n",
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#optimizer = Adam(lr=0.001, decay=0.0001)\n",
    "#model.add(Dropout(dropout_rate))\n",
    "from keras import optimizers\n",
    "adam = optimizers.Adam(lr=0.01, decay=0.01)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "num_training_series = consumption_train.series_id.nunique()\n",
    "num_passes_through_data = 3\n",
    "\n",
    "for i in tqdm(range(num_passes_through_data), \n",
    "              total=num_passes_through_data, \n",
    "              desc='Learning Consumption Trends - Epoch'):\n",
    "    \n",
    "    # reset the LSTM state for training on each series\n",
    "    for ser_id, ser_data in consumption_train.groupby('series_id'):\n",
    "\n",
    "        # prepare the data\n",
    "        X, y, scaler = prepare_training_data(ser_data.consumption, lag)\n",
    "\n",
    "        # fit the model: note that we don't shuffle batches (it would ruin the sequence)\n",
    "        # and that we reset states only after an entire X has been fit, instead of after\n",
    "        # each (size 1) batch, as is the case when stateful=False\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 51s - loss: 0.0302\n",
      "Epoch 2/10\n",
      " - 45s - loss: 0.0171\n",
      "Epoch 3/10\n",
      " - 42s - loss: 0.0166\n",
      "Epoch 4/10\n",
      " - 40s - loss: 0.0160\n",
      "Epoch 5/10\n",
      " - 40s - loss: 0.0158\n",
      "Epoch 6/10\n",
      " - 41s - loss: 0.0159\n",
      "Epoch 7/10\n",
      " - 42s - loss: 0.0160\n",
      "Epoch 8/10\n",
      " - 42s - loss: 0.0159\n",
      "Epoch 9/10\n",
      " - 41s - loss: 0.0154\n",
      "Epoch 10/10\n",
      " - 40s - loss: 0.0152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a53e1c588>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "model.fit(train_x_cust, train_y_cust, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "# plot metrics\n",
    "#pyplot.plot(history.history['mse'])\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.01 MSE (0.12 RMSE)\n",
      "Test Score: 0.08 MSE (0.29 RMSE)\n",
      "unscaled test score 723.721271 MSE\n",
      "actual test rmse  58.019325062682924\n",
      "actual train rmse  46.77313460672218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Estimate model performance\n",
    "# default batch size is 32 so we need to give batch size explicitly if we want different batch size or \n",
    "# input size should be divisible by batch size for stateful LSTM\n",
    "trainScore = model.evaluate(train_x_cust, train_y_cust, batch_size=1, verbose=2)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(test_x_cust, test_y_cust, batch_size=1, verbose=2)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "\n",
    "print('unscaled test score %f MSE' %(scalar_test.inverse_transform(testScore)))\n",
    "yhat_act_test = scalar_test.inverse_transform(testScore)\n",
    "yhat_act_train = scalar_train.inverse_transform(trainScore)\n",
    "#print (math.sqrt(scaler.inverse_transform(testScore)))\n",
    "#print (\"actual train mean_squared_error \",mean_squared_error(test_x.fv_cost[-(len(yhat_act_test)):], yhat_act_test))\n",
    "print (\"actual test rmse \",sqrt(mean_squared_error(test_x.fv_cost[-(len(yhat_act_test)):], yhat_act_test)))\n",
    "print (\"actual train rmse \",sqrt(mean_squared_error(train_x.fv_cost[-(len(yhat_act_train)):],yhat_act_train)))\n",
    "#scaler.inverse_transform(testScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch CV experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function to create model, required for KerasRegressor\n",
    "def create_model(optimizer='adam',strides_num =3,kernel_size =3,num_neurons=1,\n",
    "                 activation_func ='relu',dropout_rate = 0.2):\n",
    "    lag =  3\n",
    "    num_neurons = 24\n",
    "    batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "    batch_input_shape=(batch_size, 1, lag)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=num_neurons,batch_size=1, kernel_size=kernel_size, strides=strides_num, \n",
    "                     padding=\"same\",activation=activation_func,dilation_rate=1, input_shape=(1, 3),\n",
    "                     data_format='channels_first'))\n",
    "    model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "\n",
    "    # followed by a dense layer with a single output for regression\n",
    "    model.add(Dense(1))\n",
    "    # we can add dropoutlayer after dense as well again\n",
    "    model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=2,batch_size = 1,epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-416a07354e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_cust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_cust\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#activation_func = ['softmax', 'softplus', 'softsign', 'relu', \n",
    "#                   'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "activation_func = ['softmax', 'relu', \n",
    "                   'tanh', 'sigmoid', 'linear']\n",
    "#param_grid = dict(optimizer=optimizer,activation_func = activation_func)\n",
    "param_grid = dict(activation_func = activation_func)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_x_cust, train_y_cust)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FORECAST BIAS\n",
    "forecast_errors = [expected[i]-predictions[i] for i in range(len(expected))]\n",
    "bias = sum(forecast_errors) * 1.0/len(expected)\n",
    "print('Bias: %f' % bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a25c01710>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate predictions for training\n",
    "trainPredict = model.predict(train_x_cust,batch_size=1)\n",
    "testPredict = model.predict(test_x_cust,batch_size=1)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(df_1015130)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
    " \n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(df_1015130)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "#testPredictPlot[len(trainPredict)+(lag*2)+1:len(df_1015130)-1, :] = testPredict\n",
    " \n",
    "# plot baseline and predictions\n",
    "plt.plot(df_1015130['fv_cost'])\n",
    "plt.plot(trainPredictPlot)\n",
    "#plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100019  Test RMSE:  212.9444811085056\n"
     ]
    }
   ],
   "source": [
    "# inverse scaling for a forecasted value\n",
    "cust_id = \"100019\"\n",
    "#y_inv = invert_scale(scaler, test_x_cust, testPredict)\n",
    "yhat_act = scalar_test.inverse_transform(testPredict)\n",
    "rmse = sqrt(mean_squared_error(test_x.fv_cost[-(len(yhat_act)):], yhat_act))\n",
    "print(cust_id,\" Test RMSE: \",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions into csv\n",
    "result_prediction = pd.DataFrame()#, original_fv : test_y_cust, prediction_fv :testPredict})\n",
    "#result_prediction['dates'] = test_x['dates']\n",
    "result_prediction['original_fv'] = test_y_cust\n",
    "result_prediction['prediction_fv'] = testPredict\n",
    "\n",
    "result_prediction.to_csv(\"predictions_for_client_1015193.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-0.97603556 -0.9636714  -0.95130559 -0.93893812 -0.926569    0.29031932\n  0.30811978  0.33047664  0.34872278  0.37122721  0.39373465  0.4162451\n  0.27148751  0.29163079  0.31006937 -0.91419823 -0.91419823 -0.92099454\n -0.91649766 -0.90316826 -0.88983707 -0.8765041  -0.86316935 -0.84983281\n -0.8364945   0.33027681  0.35048694  0.37069978  0.39091532  0.41003305\n  0.43029187  0.45040125  0.46849455  0.48864712  0.50880238 -0.83230612\n -0.82476944 -0.81169946 -0.79862774 -0.78555427 -0.77431324 -0.76117315\n -0.74803129 -0.73488769 -0.72174232  0.52896034  0.54717047  0.56718363\n  0.58719947  0.60721799  0.62723918  0.64726305  0.66728959  0.68731881\n  0.70735071 -0.7085952  -0.69544632 -0.68229568 -0.66914328 -0.66025761\n -0.80006778 -0.78735201 -0.77903696 -0.76616627 -0.75329386  0.70735071\n  0.68027303  0.65163377  0.67179156  0.69195205  0.71211523  0.72563466\n  0.74513523  0.76506493  0.78279628 -0.74041972 -0.72854548 -0.71572565\n -0.70290411 -0.69008085 -0.67725588 -0.6644292  -0.6516008  -0.63950436\n -0.62702743  0.80280707  0.82282053  0.84283667  0.86285549  0.88287698\n  0.90290115  0.92292799  0.93954273  0.95969246  0.97984488 -0.61757033\n -0.60457919 -0.59158632 -0.57859171 -0.56559536 -0.57103134 -0.55880759\n -0.54677236 -0.53460875 -0.52244351  1.         -0.86533623 -0.67184821\n -0.85834319 -0.85357313 -0.84348874 -0.90140748 -0.8919321  -0.88245545\n -0.87297753 -0.51027665 -0.49810815 -0.49810815 -0.49489926 -0.49489926\n -0.53045552 -0.51788665 -0.50369568 -0.49488413 -0.48100629 -0.86349835\n -0.8643023  -0.85446661 -0.88832708 -0.87698477 -0.86717734 -0.8558912\n -0.84570406 -0.83437702 -0.82304846 -0.4671266  -0.45324505 -0.45006504\n -0.43799064 -0.42591463 -0.413837   -0.40175775 -0.3896769  -0.37759442\n -0.36551033 -0.81171839 -0.80148732 -0.79048318 -0.78251285 -0.77100867\n -0.75950296 -0.74799571 -0.76498895 -0.75910082 -0.74936347 -0.35342463\n -0.34133731 -0.32924837 -0.31715782 -0.30506565 -0.29297187 -0.2852789\n -0.27321403 -0.26114755 -0.24907945 -0.73962482 -0.72988487 -0.72014361\n -0.71552323 -0.70560308 -0.70008402 -0.69000969 -0.68402206 -0.2417036\n -0.42860998 -0.41686305 -0.41686305 -0.40998507 -0.39810499 -0.38622333\n -0.38622333 -0.45876064 -0.45876064 -0.44641151 -0.43153505 -0.41665661\n -0.41665661 -0.42090721 -0.40536633 -0.38982338 -0.37427835 -0.35873124\n -0.34391572 -0.3283392  -0.3127606  -0.29717992 -0.28159715 -0.26640711\n -0.25083336 -0.28019197 -0.26423943 -0.25843251 -0.24212643 -0.22581818\n -0.20950775 -0.21140346 -0.20063254 -0.1834764  -0.22604758 -0.20900643\n -0.191963   -0.1749173  -0.16227174 -0.20381442 -0.1855679  -0.16731894\n -0.14906754 -0.13081371 -0.13800624 -0.11984827 -0.10168786 -0.08352503\n -0.08352503 -0.08352503 -0.06854807 -0.05283173 -0.03446371 -0.01609324\n  0.00227969  0.01845404  0.03617396  0.05465527  0.07313906  0.09162532\n  0.11011404  0.12860524  0.14298458  0.16162233  0.17806167  0.19678016\n  0.21550115  0.23422464  0.25295063  0.27167913  0.27167913  0.28173922\n  0.28173922  0.2879614   0.2637393   0.28472128  0.30570605  0.32669363\n  0.34768402  0.36867721  0.33795662  0.35499993  0.37756     0.40012308\n  0.40012308  0.41623729  0.43902847  0.43902847  0.43902847  0.45885448\n  0.48175391  0.50465641  0.52756197  0.55047059  0.57082118  0.59382407\n  0.61683004  0.63561324  0.65783701  0.32191552  0.34810545  0.34810545\n  0.37476622  0.27208782  0.28606965  0.25390692  0.27113696  0.29180496\n  0.26877825].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f0096dad78da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscalar_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_cust\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-0.97603556 -0.9636714  -0.95130559 -0.93893812 -0.926569    0.29031932\n  0.30811978  0.33047664  0.34872278  0.37122721  0.39373465  0.4162451\n  0.27148751  0.29163079  0.31006937 -0.91419823 -0.91419823 -0.92099454\n -0.91649766 -0.90316826 -0.88983707 -0.8765041  -0.86316935 -0.84983281\n -0.8364945   0.33027681  0.35048694  0.37069978  0.39091532  0.41003305\n  0.43029187  0.45040125  0.46849455  0.48864712  0.50880238 -0.83230612\n -0.82476944 -0.81169946 -0.79862774 -0.78555427 -0.77431324 -0.76117315\n -0.74803129 -0.73488769 -0.72174232  0.52896034  0.54717047  0.56718363\n  0.58719947  0.60721799  0.62723918  0.64726305  0.66728959  0.68731881\n  0.70735071 -0.7085952  -0.69544632 -0.68229568 -0.66914328 -0.66025761\n -0.80006778 -0.78735201 -0.77903696 -0.76616627 -0.75329386  0.70735071\n  0.68027303  0.65163377  0.67179156  0.69195205  0.71211523  0.72563466\n  0.74513523  0.76506493  0.78279628 -0.74041972 -0.72854548 -0.71572565\n -0.70290411 -0.69008085 -0.67725588 -0.6644292  -0.6516008  -0.63950436\n -0.62702743  0.80280707  0.82282053  0.84283667  0.86285549  0.88287698\n  0.90290115  0.92292799  0.93954273  0.95969246  0.97984488 -0.61757033\n -0.60457919 -0.59158632 -0.57859171 -0.56559536 -0.57103134 -0.55880759\n -0.54677236 -0.53460875 -0.52244351  1.         -0.86533623 -0.67184821\n -0.85834319 -0.85357313 -0.84348874 -0.90140748 -0.8919321  -0.88245545\n -0.87297753 -0.51027665 -0.49810815 -0.49810815 -0.49489926 -0.49489926\n -0.53045552 -0.51788665 -0.50369568 -0.49488413 -0.48100629 -0.86349835\n -0.8643023  -0.85446661 -0.88832708 -0.87698477 -0.86717734 -0.8558912\n -0.84570406 -0.83437702 -0.82304846 -0.4671266  -0.45324505 -0.45006504\n -0.43799064 -0.42591463 -0.413837   -0.40175775 -0.3896769  -0.37759442\n -0.36551033 -0.81171839 -0.80148732 -0.79048318 -0.78251285 -0.77100867\n -0.75950296 -0.74799571 -0.76498895 -0.75910082 -0.74936347 -0.35342463\n -0.34133731 -0.32924837 -0.31715782 -0.30506565 -0.29297187 -0.2852789\n -0.27321403 -0.26114755 -0.24907945 -0.73962482 -0.72988487 -0.72014361\n -0.71552323 -0.70560308 -0.70008402 -0.69000969 -0.68402206 -0.2417036\n -0.42860998 -0.41686305 -0.41686305 -0.40998507 -0.39810499 -0.38622333\n -0.38622333 -0.45876064 -0.45876064 -0.44641151 -0.43153505 -0.41665661\n -0.41665661 -0.42090721 -0.40536633 -0.38982338 -0.37427835 -0.35873124\n -0.34391572 -0.3283392  -0.3127606  -0.29717992 -0.28159715 -0.26640711\n -0.25083336 -0.28019197 -0.26423943 -0.25843251 -0.24212643 -0.22581818\n -0.20950775 -0.21140346 -0.20063254 -0.1834764  -0.22604758 -0.20900643\n -0.191963   -0.1749173  -0.16227174 -0.20381442 -0.1855679  -0.16731894\n -0.14906754 -0.13081371 -0.13800624 -0.11984827 -0.10168786 -0.08352503\n -0.08352503 -0.08352503 -0.06854807 -0.05283173 -0.03446371 -0.01609324\n  0.00227969  0.01845404  0.03617396  0.05465527  0.07313906  0.09162532\n  0.11011404  0.12860524  0.14298458  0.16162233  0.17806167  0.19678016\n  0.21550115  0.23422464  0.25295063  0.27167913  0.27167913  0.28173922\n  0.28173922  0.2879614   0.2637393   0.28472128  0.30570605  0.32669363\n  0.34768402  0.36867721  0.33795662  0.35499993  0.37756     0.40012308\n  0.40012308  0.41623729  0.43902847  0.43902847  0.43902847  0.45885448\n  0.48175391  0.50465641  0.52756197  0.55047059  0.57082118  0.59382407\n  0.61683004  0.63561324  0.65783701  0.32191552  0.34810545  0.34810545\n  0.37476622  0.27208782  0.28606965  0.25390692  0.27113696  0.29180496\n  0.26877825].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "scalar_test.inverse_transform(test_y_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions for timesteps ahead\n",
    "# 7,14 ,21 timesteps ahead\n",
    "#we need atleast 4 elements to prepare training data\n",
    "futureElement = testPredict[-4:]\n",
    "\n",
    "def prepare_training_data_array(series_data, lag):\n",
    "    \" Converts a series of data into a lagged, scaled sample.\"\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    cost_vals = scaler.fit_transform(series_data.reshape(-1, 1))\n",
    "    \n",
    "    # convert series to lagged features\n",
    "    cost_lagged = lag_feature(cost_vals, lag=lag)\n",
    "\n",
    "    # X, y format taking the first column (original time series) to be the y\n",
    "    X = cost_lagged.drop('fv_cost', axis=1).values\n",
    "    y = cost_lagged.fv_cost.values\n",
    "    \n",
    "    # keras expects 3 dimensional X\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape)\n",
    "x=futureElement[:3]\n",
    "#def timestep_ahead(num,model,futureElement):\n",
    "#futureElement = futureElement_x\n",
    "def timestep_ahead(num,model,futureElement,x):\n",
    "    futureElements = []\n",
    "    futureElements.append(futureElement.tolist())\n",
    "\n",
    "    for i in range(num):\n",
    "        futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "        print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "        print(\"future element value after preparing data\",futureElement_x)\n",
    "        futureElement = model.predict(futureElement_x,batch_size=1,verbose=2)\n",
    "        futureElements.append(futureElement)\n",
    "        print(\"future element before appending x\",futureElement)\n",
    "        futureElement = np.append(futureElement,x)\n",
    "        x=futureElement[:3]\n",
    "        print(\"future element after appending x\",futureElement)\n",
    "        print(\"iteration \",i,\"  complete\")\n",
    "    return (futureElement[0],futureElements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(futureElements[0][0][0][:2])\n",
    "\n",
    "#future_x = np.array(futureElement,futureElements[0][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_7time_ahead,elements = timestep_ahead(7,model,futureElement,x)\n",
    "print (\"scaled prediction value on 7th day ahead \",_7time_ahead)\n",
    "_14time_ahead,elements  = timestep_ahead(14,model,futureElement,x)\n",
    "print (\"scaled prediction value on 14th day ahead \",_14time_ahead)\n",
    "_21time_ahead,elements  = timestep_ahead(21,model,futureElement,x)\n",
    "print (\"scaled prediction value on 21st day ahead \",_21time_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3months ahead predicitons\n",
    "_91days_ahead,elements = timestep_ahead(91,model,futureElement,x)\n",
    "print (\"scaled prediction value on 91st day ahead \",_91days_ahead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pred =[np.array(arr).tolist() for arr in elements]\n",
    "daily_pred\n",
    "def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]\n",
    "pred_list = flatten(daily_pred)\n",
    "\n",
    "import csv \n",
    "myFile = open('daily_predictions_3months.csv', 'w')  \n",
    "with myFile:  \n",
    "   writer = csv.writer(myFile)\n",
    "   writer.writerow(pred_list)\n",
    "\n",
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find every 7th element into a list\n",
    "week_end_pred = pred_list[0::7]\n",
    "week_end_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly average for 3 months\n",
    "weekly_avg_pred = [(sum(pred_list[x:x+7]))/7 for x in range(0, len(pred_list), 7)]\n",
    "weekly_avg_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find average for every 7 days\n",
    "import itertools\n",
    "n=7 #7days\n",
    "list_week = list(itertools.chain.from_iterable([i]*7 for i in [sum(pred_list[i:i+7])//7 for i in range(0,len(pred_list),7)]))\n",
    "list_week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for i in range(7):\n",
    "futureElement = model.predict(futureElement,batch_size=1)\n",
    "futureElements.append(futureElement)\n",
    "futureElement = np.append(futureElement, futureElements[0][0][0][:3])\n",
    "x=futureElement[:3]\n",
    "print(x)\n",
    "print(\"iteration 0 complete\")\n",
    "#    #return futureElements\n",
    "print(futureElement)\n",
    "print(futureElements)\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "print(\"future element value after preparing data\",futureElement_x)\n",
    "futureElement = model.predict(futureElement_x,batch_size=1)\n",
    "futureElements.append(futureElement)\n",
    "print(\"future element before appending x\",futureElement)\n",
    "futureElement = np.append(futureElement,x)\n",
    "x=futureElement[:3]\n",
    "print(\"future element after appending x\",futureElement)\n",
    "print(\"iteration 1 complete\")\n",
    "print(futureElements)\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "print(\"future element value after preparing data\",futureElement_x)\n",
    "futureElement = model.predict(futureElement_x,batch_size=1)\n",
    "futureElements.append(futureElement)\n",
    "print(\"future element before appending x\",futureElement)\n",
    "futureElement = np.append(futureElement,x)\n",
    "x=futureElement[:3]\n",
    "print(\"future element after appending x\",futureElement)\n",
    "print(\"iteration 2 complete\")\n",
    "print(futureElements)\n",
    "\n",
    "print(type(futureElements))\n",
    "print(type(futureElement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [x for sublist in futureElements for x in sublist]\n",
    "print(flat_list[0][0][1])\n",
    "\n",
    "#above function loops in below format\n",
    "#for sublist in futureElements:\n",
    "#    for x in sublist:\n",
    "#        flat_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price.resample('1W')  \n",
    "#aapl.resample('W').mean()\n",
    "#http://benalexkeen.com/resampling-time-series-data-with-pandas/\n",
    "#resampling based on days and then slicing every 7th day\n",
    "#ts.resample('D').interpolate()[::7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.last('3M') #last 3 months data\n",
    "# to sort dates column before getting last 3months\n",
    "#df_sorted = df.sort_values(by=\"Date\",ascending=True) \\\n",
    "#    .set_index(\"Date\")\n",
    "#    .last(\"3M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/305863/how-to-train-lstm-model-on-multiple-time-series-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/67362/shall-i-use-weekly-or-monthly-data-for-forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weekly predicitons\n",
    "#weekly data\n",
    "week_data = df_1015130.set_index('dates').resample('1W').mean()\n",
    "#week_data['dates'] = pd.to_datetime(week_data['dates'], errors='coerce')\n",
    "\n",
    "\n",
    "week_data = week_data.resample('1W')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#week_data['dates'] = pd.to_datetime(week_data['dates'], unit='D',utc=True)\n",
    "week_data = df_1015130\n",
    "##week_data['New']=week_data.dates.map(week_data.set_index('dates').iloc[1:].resample('D').sum().rolling(7,min_periods =1).visit.mean()).shift()\n",
    "week_data['dates'] = pd.to_datetime(week_data['dates'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data.set_index('dates',inplace=True)\n",
    "week_data_resample = week_data.resample('1W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data_resample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_index = week_data_resample.index(\"/2018\")\n",
    "#l2 = l[:c_index]\n",
    "week_data_resample['dates'] = week_data_resample.index\n",
    "week_data_resample['dates']=week_data_resample['dates'].astype(str)\n",
    "week_data_resample = week_data_resample.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_week = week_data_resample[week_data_resample['dates'].str.contains('2018') == False]\n",
    "test_week = week_data_resample[week_data_resample['dates'].str.contains('2018') == True]\n",
    "\n",
    "train_x_week,train_y_week, scalar_train = prepare_training_data(train_week['fv_cost'], 3)\n",
    "test_x_week,test_y_week,scalar_test = prepare_training_data(test_week['fv_cost'], 3)\n",
    "print(train_x_week.shape)\n",
    "print(train_y_week.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag =  3\n",
    "num_neurons = 24\n",
    "batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=24,batch_size=1, kernel_size=3, strides=3, padding=\"same\",activation='relu',dilation_rate=1, input_shape=(1, 3),data_format='channels_first'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x_week, train_y_week, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScore = model.evaluate(train_x_week, train_y_week, batch_size=1, verbose=2)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(test_x_week, test_y_week, batch_size=1, verbose=2)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "print('unscaled test score %f MSE' %(scalar_test.inverse_transform(testScore)))\n",
    "scalar_train.inverse_transform(trainScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for training\n",
    "trainPredict = model.predict(train_x_week,batch_size=1)\n",
    "testPredict = model.predict(test_x_week,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futureElement = testPredict[-4:]\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape)\n",
    "x=futureElement[:3]\n",
    "#def timestep_ahead(num,model,futureElement):\n",
    "#futureElement = futureElement_x\n",
    "def timestep_ahead_week(num,model,futureElement,x):\n",
    "    futureElements = []\n",
    "    futureElements.append(futureElement.tolist())\n",
    "\n",
    "    for i in range(num):\n",
    "        futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "        print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "        print(\"future element value after preparing data\",futureElement_x)\n",
    "        futureElement = model.predict(futureElement_x,batch_size=1,verbose=2)\n",
    "        futureElements.append(futureElement)\n",
    "        print(\"future element before appending x\",futureElement)\n",
    "        futureElement = np.append(futureElement,x)\n",
    "        x=futureElement[:3]\n",
    "        print(\"future element after appending x\",futureElement)\n",
    "        print(\"iteration \",i,\"  complete\")\n",
    "    return (futureElement[0],futureElements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_12_weeks_ahead,all_pred = timestep_ahead_week(12,model,futureElement,x)\n",
    "print (\"scaled prediction value on 3rd month ahead \",_12_weeks_ahead)\n",
    "print (\"all predicitons \",all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of arrays to lists\n",
    "#l = [arr.tolist() for arr in l]\n",
    "#[l.tolist() for l in list1]\n",
    "\n",
    "#weekly_pred = [arr.tolist() for arr in all_pred] #list object has no attribute tolist\n",
    "weekly_pred =[np.array(arr).tolist() for arr in all_pred]\n",
    "weekly_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = flatten(weekly_pred)\n",
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvwriter.writerow(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "myFile = open('weekly_predictions_3months.csv', 'w')  \n",
    "with myFile:  \n",
    "   writer = csv.writer(myFile)\n",
    "   writer.writerow(pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiemnt code for outlier behaving customer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions and analysis for customer with highest rmse 1681085\n",
    "df1 = df[df.fv_cost.notnull()]\n",
    "cust_data = df1[df1['client_debtor_number'] == 1681085]\n",
    "del cust_data['client_debtor_number']\n",
    "\n",
    "\n",
    "lag =  3\n",
    "\n",
    "# model parameters\n",
    "num_neurons = 24\n",
    "batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=24,batch_size=1, kernel_size=3, strides=3, padding=\"same\",activation='relu',dilation_rate=1, input_shape=(1, 3),data_format='channels_first'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "model.add(Dense(1))\n",
    "# we can add dropoutlayer after dense as well again\n",
    "model.add(Dropout(0.2))\n",
    "# compile\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "train_x = cust_data[cust_data['dates'].str.contains('/2018') == False]\n",
    "test_x = cust_data[cust_data['dates'].str.contains('/2018') == True]\n",
    "print(train_x.head(), len(train_x))\n",
    "print (test_x.head(),len(test_x))\n",
    "\n",
    "train_x_cust,train_y_cust, scalar_train = prepare_training_data(train_x['fv_cost'], 3)\n",
    "test_x_cust,test_y_cust,scalar_test = prepare_training_data(test_x['fv_cost'], 3)  \n",
    "model.fit(train_x_cust, train_y_cust, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    # input size should be divisible by batch size for stateful LSTM\n",
    "trainScore = model.evaluate(train_x_cust, train_y_cust, batch_size=1, verbose=2)\n",
    "testScore = model.evaluate(test_x_cust, test_y_cust, batch_size=1, verbose=2)\n",
    "train_rmse = trainScore ###\n",
    "test_rmse = testScore ###\n",
    "    # generate predictions for training\n",
    "trainPredict = model.predict(train_x_cust,batch_size=1)\n",
    "testPredict = model.predict(test_x_cust,batch_size=1)\n",
    "yhat_act_test = scalar_test.inverse_transform(testPredict)\n",
    "    # report performance\n",
    "rmse_test = sqrt(mean_squared_error(test_x.fv_cost[-(len(yhat_act_test)):], yhat_act_test))\n",
    "print(cust_id,\" Test RMSE: \",rmse_test)\n",
    "actual_error_scores_test = rmse_test ####\n",
    "yhat_act_train = scalar_train.inverse_transform(trainScore)\n",
    "rmse_train = sqrt(mean_squared_error(train_x.fv_cost[-(len(yhat_act_train)):],yhat_act_train))\n",
    "actual_train_rmse = rmse_train #####\n",
    "    # write predictions into csv\n",
    "result_prediction = pd.DataFrame()#, original_fv : test_y_cust, prediction_fv :testPredict})\n",
    "    #result_prediction['dates'] = test_x['dates']\n",
    "result_prediction['original_fv_scaled'] = test_y_cust\n",
    "result_prediction['prediction_scaled'] = testPredict\n",
    "result_prediction['prediction_actual'] = yhat_act_test\n",
    "result_prediction['fv_actual'] = test_x.fv_cost[-(len(yhat_act_test)):]\n",
    "result_prediction.to_csv(\"predictions_1/predictions_for_client_1681085.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
